# SFO Distribution Artifact
We collected certificate chains and SCTs (or so-called SFOs) by browsing the
most popular webpages that were submitted to Reddit (r/frontpage, all time) as
of December 4, 2019.  Below you will find further instructions on how to
reproduce the dataset today and/or download it as observed by us on February 26,
2020.

## Reproduce raw and parsed dataset
We used a headless VM running Ubuntu Desktop 19.10 and Chromium 77.  You can get
the latter by downloading `chrome-linux.zip` from
[here](https://commondatastorage.googleapis.com/chromium-browser-snapshots/index.html?prefix=Linux_x64/681090/)
and then unzipping it as a directory named `77`.  We also installed the
following dependencies: `apt install xvfb dbus-x11 python3`.

The list of webpages that we visited can be found
[here](https://github.com/pylls/padding-machines-for-tor/blob/master/collect-traces/lists/unmonitored/reddit-front-all.list):
save it as `reddit-front-all.list`.  Each line must be a single URL, and it
is skipped unless it starts with the string `https://`.

Now that the environment is setup and we have a list of webpages to browse,
we can do so by running the `collect` script.
```
$ ./collect 2> collect.log
```

Each webpage visit results in a separate NetLog dump in the newly created `raw`
directory.  This is what we parse later on to extract the presented certificate
chains and SCTs. As you can see from the example output below, the generated log
file tells you which data point corresponds to which webpage.
```
$ cat collect.log
[Info] 1 https://example.com
[Info] 2 https://kau.se
[Info] 3 https://no-sct.badssl.com
$ ls raw
1 2 3
```

The raw dataset can be parsed by running the `parse` script.   It basically
loops over all NetLog dumps in `raw`, running `parse.py` for each of them
and adding the result in the existing or newly created directory `parsed`.
```
$ grep '^#' parse.py
#!/usr/bin/env python3
#
# usage: cat example.com | ./parse > example.com.json
#
# Here, example.com is a NetLog dump as generated by the collect script.  The
# resulting output is a json-encoded list where each item in turn lists:
# 1) Number of certificates in this certificate chain
# 2) Number of wire-bytes in this certificate chain
# 3) Number of presented SCTs
# 4) End-entity serial number
# 5) End-entity common name
#
# You may find Chromium's NetLog documentation helpful:
# https://www.chromium.org/developers/design-documents/network-stack/netlog
#
# For example, all NetLog events are enumerated here:
# https://cs.chromium.org/chromium/src/net/log/net_log_event_type_list.h
#
$ ./parse
[Info] done
$ ls parsed
1.json  2.json  3.json
```

## Download raw and parsed dataset
The raw dataset is hosted over [here: TODO fix me]().
Download and unzip, which gives you a directory named `share`.  It contains the
files and directories that the above steps generated.

## Derive dataset values from ยง7.1
First install numpy (`apt install python-pip && pip install numpy`), then run
`view.py` as shown below.
```
$ [[ -d parsed ]] && ./view.py
[INFO] avg num of sfos per website: 7.0
[INFO] Avg chain len: 5439.8 bytes
[INFO] Max chain len: 20218.0 bytes
[INFO] Avg num of SCTs per chain: 2.2
```
