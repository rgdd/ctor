Dear reviewers,

Thank you for your valuable comments.  We considered all of them, and structured
our summary-of-change document according to the PETS resubmission guidelines.

1.  Summary of changes

We used the feedback provided by all reviewers to clarify the paper and better
capture its context.  The parts that changed only marginally or not at all are:
- The initial paragraphs leading up to §1.1
- Background on Tor (§2.2)
- The second half of our threat model (§3)
- Privacy discussion (§8)
- Related work (§9)

The remaining parts received major revisions that followed from the meta review
and other minor comments.  We present these revisions in detail subsequently.

The first three sections of the paper capture additional details on CT and the
intended attacker.  The order in which we present our work is then flipped,
explaning the full design and how we engineered it step-by-step; followed by its
security analysis.  This made it easier to justify the need for initial
increments that can be rolled-out gradually, motivating cross-logging designs.
Finally, the performance section received a refactor that clarifies the setup
(public data sets and parameters) as well as the formulas used to compute our
estimates.  Our previous analysis and presentation may have made it seem that a
simulation would strengthen confidence in CTor's performance. This refactored
analysis should make unambiguous that CTor's performance overhead is acceptable.

2.  Meta review

2.1.  Critical evaluation of the current CT deployment

A short justification of the focus on the current trust-based CT deployment
appears in the introduction, and an extended justification follows in the
background: we can be certain that the user experience remains about the same by
starting from the assumption of pairwisely trusted CT logs that other major web
browsers already deployed.  This makes it easier to motivate initial deployment
in Tor Browser.  CTor integrates into the existing CT ecosystem by retaining
these desirable properties, but additionally protects against increasingly
powerful attackers.

The second part of our critical evaluation of the current CT deployment is
in-lined as context throughout our designs (§4, §6).  We decided to restructure
the paper to first present the full design that places zero-trust in CT logs,
motivating its complexity by showcasing straw-man designs that are intuitive but
do not work.  The lack of reliable CT auditors as well as complexity makes it
difficult to deploy all at once, which calls for initial increments that can
be rolled-out gradually.  Therefore, we proposed designs based on cross-logging.
The impact that these increments have on the status quo is mentioned in §6.

2.2.  Insight and effect of configurable parameters

We added additional formulas and explanations in the performance section.  This
clarifies how our estimates were computed, and makes it easier for the reader to
try a different set of parameters.  The evaluated parameters are also motivated
better, e.g., the relatively low submission probability follows from our threat
model: the attacker is risk-averse and would not gamble with non-negligible
probabilities of detection.  If a subset of users are targeted, even a low
submission probability quickly implies high certainty that someone submits an
SFO as well.

2.3.  Data set and possible biases

We added additional details on our setup in a separate subsection (§7.1).  The
parsed SFO data set and all scripts will be online as an open access artifact.

In short, the collected data set does not use Alexa's top-domains because we
were afraid that front-pages refer to fewer resources (and thus less SFOs).
This hypothesis was confirmed but not mentioned in the previous submission.

The fact that we collected a relatively realistic SFO data set and used the
upper bound of a confidence interval from Mani et al. is paramount for our
evaluation: it reduces the risk of performance underestimates.

2.4.  Motivation of threat model

The introduction now emphasizes why the three main attacker capabilities are
realistic.  Examples of attacker incentives are also detailed further in §3.

2.5.  Improved readability and conceivable structure

We decided to restructure the paper to better capture its context.  The full
design that places zero-trust in CT logs is described first, motivating its
complexity and components step-by-step to give the reader a better intuition.

After arriving at a full design, we analyze its security, followed by deployment
barriers that motivate the incremental cross-logging designs.

This structure emphasizes the full design and the thought that went into it,
including the incremental parts and how they improve upon the status quo.

3.  Individual reviews

3.1.  Review#A

We agree that CT logs should not be trusted, but the gap between theory and
practise is quite large.  The previous submission failed to capture this.

The updated paper structure kept the comments in this review in mind by
emphasizing the full design that places zero-trust in CT logs, as well as
the overall context that justifies its complexity, our starting point, and the
need for initial increments that gradually reduce trust in today's CT logs.

Detailed comments:
- Voting on constants can be valuable as a reaction to network events.  Examples
are provided in the updated paper, see the final paragraphs of §5.1.2 and §10.
- Certificate cross-logging can in fact speed-up detection of certificate
mis-issuance (if a benign log is sampled).  We clarified this as part of §6.

3.2.  Review#B

The updated performance section kept the comments in this review in mind by
adding additional explanations and formulas.  This makes it easier for the
reader to understand the computed estimates and try different parameters.

The estimated impact of caching was added per suggestion, as well as overhead
changes between the initial cross-logging increments and the full design.

Detailed comments:
- The check whether two logs are independent is not an online process: it can
be inferred from the SCTs that accompany a certificate chain.  This is probably
clearer now that the CT enforcements of Chrome and Safari are emphasized more.
- Answers regarding today's CT log ecosystem can be found in Appendix B.
- The intended attacker is motivated better (§1.1, §3).

3.3.  Review#C

The comments in this review go hand-in-hand with the others.

Detailed comments:
- No usability evaluation is needed because Tor Browser uses an SCT-centric CT
policy similar to Chrome and Safari (that does not break the web).  Similarly, a
relay operator need not know anything about CT to run a CTR (assigned by flags).
- Gain control of Tor Browser 'shortly' means as soon as the attacker's exploit
code can load and escalate privileges.  It is nontrivial to provide an exact
time, but some intuition regarding the resulting race was added in §4.1.
- There is no CT enforcement in Tor Browser currently.  Clarified in §1.1.
- Some acronyms were removed, e.g., TB is now spelled out as Tor Browser.

3.4.  Review#D

The comments in this review go hand-in-hand with the others.  The major points
were addressed by restructuring the design (§4, §6), adding data set details
(§7.1), and motivating the intended attacker further (§1.1, §3).

Detailed comments:
- The last bullet point of Appendix A (now B) was removed while refactoring.
- The delete-at-random strategy is used because everything else we tried
provided an advantage to the attacker while attempting to flush.  We cited
prior work that also used delete-at-random, probably due to similar reasons.

4.  Disagreements

We believe that our updated performance analysis proves the point we are trying
to make: CTor's overhead is acceptable.  Therefore, there is no simulation.

5.  Other changes

Many minor edits, none of which are detailed here.
