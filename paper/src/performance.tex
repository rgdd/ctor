\section{Performance} \label{sec:performance}
The following analysis shows that CTor's overhead is modest based on computing
performance estimates from concrete parameter properties and two public data
sets.

\subsection{Setup}
Mani~\emph{et~al.} derived a distribution of website visits over Tor and an
estimation of the number of circuits through the network~\cite{mani}.  We use
their results to reason about overhead as the Tor network is under heavy load,
assuming 140~million daily website visits (the upper bound of a 95\% confidence
interval).  Our analysis also requires a distribution that captures typical SFO
properties per website visit.  Therefore, we collected an SFO data set by
browsing the most popular webpages submitted to Reddit (r/frontpage, all time)
on December 4, 2019.  The data set contains SFOs from 8858 webpage visits, and
it is available online as an open access artifact together with the associated
scripts~\cite{sfo-dist}.  Notably we hypothesized that browsing actual webpages
as opposed to front-pages would yield more SFOs.  When compared to Alexa's
list it turned out to be the case:
	our data set has roughly two additional SFOs per data point.
This makes it less likely that our analysis is an underestimate.

We found that an average certificate chain is 5440~bytes, and it is seldom
accompanied by more than a few SCTs.  As such, a typical SFO is in the order of
6~KiB.  No certificate chain exceeded 20~KiB, and the average number of SFOs per
webpage was seven.  The latter includes 1--2 SFOs per data point that followed
from our client software calling home on start-up (Chromium~77).

We assume no abnormal CTor behavior, which means that there will be little or
no CTR back-offs due to the high uptime requirements of today's CT logs: 99\%.
We set \texttt{ct-large-sfo-size} conservatively to avoid blocking in the TLS
handshake (e.g., 20~KiB), and use a 10\% submission probability as well as a
10~minute random buffer delay on average.  It is likely unwarranted to use a
higher submission probability given that the intended attacker is risk-averse.
Shorter buffer times would leak finer-grained browsing patterns to the logs,
while longer ones increase the attack surface in phase~2.  Therefore, we
selected an average for \texttt{ct-delay-dist} that satisfies none of the two
extremes.  The remaining CTor parameters are timeouts, which have little or no
performance impact if set conservatively (few seconds).

\subsection{Estimates} \label{sec:performance:estimates}
The incremental cross-logging designs are analyzed first without any caching.
Caching is then considered, followed by overhead that appears only in the full
design.

\textbf{Circuit overhead.}
Equation~\ref{eq:sub-oh} shows the expected circuit overhead from Tor Browser
over time, where $p$ is the submit probability and $\bar{d}$ the average number
of SFOs per website visit.  The involved overhead is linear as either of the two
parameters are tuned up or down.

\begin{equation} \label{eq:sub-oh}
	p\bar{d}
\end{equation}

Using $p\gets\frac{1}{10}$ and our approximated SFO distribution $\bar{d}\gets7$
yields an average circuit overhead of $0.70$, i.e., for every three Tor Browser
circuits CTor adds another two.  Such an increase might sound
daunting at first,\footnote{%
	Circuit establishment involves queueing of onionskins~\cite{onionskins} and
	it is a likely bottleneck, but since the introduction of ntor it is not a
	scarce resource so such overhead is acceptable if it (i) serves a purpose,
	and (ii) can be tuned.  Confirmed by Tor developers.
} but these additional circuits are short-lived and light-weight; transporting
6~KiB on average.  Each CTR also maintains a long-lived circuit for CT log
interactions.

\textbf{Bandwidth overhead.}  Equation~\ref{eq:bw} shows the expected
bandwidth overhead for the Tor network over time, where
	$V$ is the number of website visits per time unit,
	$p$ the submit probability,
	$\bar{d}$ the average number of SFOs per website visit, and
	$\bar{s}$ the average SFO byte-size.

\begin{equation} \label{eq:bw}
	6Vp\bar{d}\bar{s}
\end{equation}

$Vp\bar{d}$ is the average number of SFO submissions per time unit, which can be
converted to bandwidth by weighting each submission with the size of
a typical SFO and accounting for it being relayed six times:
	three hops from Tor Browser to a CTR, then
	another three hops from the CTR to a CT log
	(we assumed symmetric Tor relay bandwidth).
Using
	$V\gets 140\textrm{~M/day}$,
	$p \gets \frac{1}{10}$,
	$\bar{d} \gets 7$,
	$\bar{s} \gets 6\textrm{~KiB}$
and converting the result to bps yields 334.5~Mbps in total.  Such order of
overhead is small when compared to Tor's capacity:
450~Gbps~\cite{tor-bandwidth}.

\textbf{Memory overhead.}
Equation~\ref{eq:memory} shows the expected buffering overhead, where
	$V_m$ is the number of website visits per minute,
	$t$ the average buffer time in minutes,
	$R$ the number of Tor relays that qualify as CTRs, and
	$\bar{s}$ the typical SFO size in bytes.

\begin{equation} \label{eq:memory}
	\frac{V_mt}{R} \bar{s}
\end{equation}

$V_mt$ represent incoming SFO submissions during the average buffer time, which
are randomly distributed across $R$ CTRs.  Combined, this yields the expected
number of SFOs that await at a single CTR in phase~2, and by taking the
byte-size of these SFOs into account we get an estimate of the resulting memory
overhead.  Using
	$V_m \gets \frac{140\textrm{~M}}{24\cdot60}$,
	$t \gets 10$~m,
	$R \gets 4000$ based on the CTR criteria in
		Section~\ref{sec:base:consensus}, and
	$\bar{s} \gets 6\textrm{~KiB}$
yields 1.42~MiB.  Such order of overhead is small when compared to the
recommended relay configuration:
	at least 512~MiB~\cite{relay-config}.

A cache of processed SFOs reduces the CTR's buffering memory and log
interactions proportionally to the cache hit ratio.  Mani~\emph{et al.} showed
that if the overrepresented \texttt{torproject.org} is removed, about one third
of all website visits over Tor can be attributed to Alexa's top-1k and another
one third to the top-1M~\cite{mani}.
Assuming 32~byte cryptographic hashes and seven SFOs per website visit, a cache
hit ratio of $\frac{1}{3}$ could be achieved by a 256~KiB LFU/LRU cache that
eventually captures Alexa's top-1k.  Given that the cache requires memory as
well, this is mainly a bandwidth optimization.

\textbf{Full design.}
For each CTR and CT log pair, there is an additional watchdog circuit that
transports the full SFO upfront before fetching an inclusion proof.  The
expected bandwidth overhead is at most $9Vp\bar{d}\bar{s}$, i.e., now
also accounting for the three additional hops that an SFO is subject to.  In
practise the overhead is slightly less, because an inclusion query and its
returned proof is smaller than an SFO.  We expect little or no
watchdog-to-auditor overhead if the logs are available, and otherwise one
light-weight circuit that reports a single SFO for each CTR that goes into
back-off.  Such overhead is small when compared to all Tor Browser submissions.
Finally, the required memory increases because newly issued SFOs are buffered
for at least an MMD.  Only a small portion of SFOs are newly issued, however:
	the short-lived certificates of Let's Encrypt are valid for
	90~days~\cite{le}, which is in contrast to 24~hour
	MMDs~\cite{google-log-policy}.
